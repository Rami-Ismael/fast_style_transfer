{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SetUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import functools\n",
    "import os\n",
    "\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "print(\"TF Version: \", tf.__version__)\n",
    "print(\"TF Hub version: \", hub.__version__)\n",
    "print(\"Eager mode enabled: \", tf.executing_eagerly())\n",
    "print(\"GPU available: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define image loading and visualization functions  { display-mode: \"form\" }\n",
    "\n",
    "def crop_center(image):\n",
    "  \"\"\"Returns a cropped square image.\"\"\"\n",
    "  shape = image.shape\n",
    "  new_shape = min(shape[1], shape[2])\n",
    "  offset_y = max(shape[1] - shape[2], 0) // 2\n",
    "  offset_x = max(shape[2] - shape[1], 0) // 2\n",
    "  image = tf.image.crop_to_bounding_box(\n",
    "      image, offset_y, offset_x, new_shape, new_shape)\n",
    "  return image\n",
    "\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def load_image(image_url, image_size=(256, 256), preserve_aspect_ratio=True):\n",
    "  \"\"\"Loads and preprocesses images.\"\"\"\n",
    "  # Cache image file locally.\n",
    "  image_path = tf.keras.utils.get_file(os.path.basename(image_url)[-128:], image_url)\n",
    "  # Load and convert to float32 numpy array, add batch dimension, and normalize to range [0, 1].\n",
    "  img = tf.io.decode_image(\n",
    "      tf.io.read_file(image_path),\n",
    "      channels=3, dtype=tf.float32)[tf.newaxis, ...]\n",
    "  img = crop_center(img)\n",
    "  img = tf.image.resize(img, image_size, preserve_aspect_ratio=True)\n",
    "  return img\n",
    "\n",
    "def show_n(images, titles=('',)):\n",
    "  n = len(images)\n",
    "  image_sizes = [image.shape[1] for image in images]\n",
    "  w = (image_sizes[0] * 6) // 320\n",
    "  plt.figure(figsize=(w * n, w))\n",
    "  gs = gridspec.GridSpec(1, n, width_ratios=image_sizes)\n",
    "  for i in range(n):\n",
    "    plt.subplot(gs[i])\n",
    "    plt.imshow(images[i][0], aspect='equal')\n",
    "    plt.axis('off')\n",
    "    plt.title(titles[i] if len(titles) > i else '')\n",
    "  plt.show()\n",
    "# Function to load an image from a file, and add a batch dimension.\n",
    "def load_img(path_to_img):\n",
    "  img = tf.io.read_file(path_to_img)\n",
    "  img = tf.io.decode_image(img, channels=3)\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  img = img[tf.newaxis, :]\n",
    "\n",
    "  return img\n",
    "\n",
    "# Function to pre-process by resizing an central cropping it.\n",
    "def preprocess_image(image, target_dim):\n",
    "  # Resize the image so that the shorter dimension becomes 256px.\n",
    "  shape = tf.cast(tf.shape(image)[1:-1], tf.float32)\n",
    "  short_dim = min(shape)\n",
    "  scale = target_dim / short_dim\n",
    "  new_shape = tf.cast(shape * scale, tf.int32)\n",
    "  image = tf.image.resize(image, new_shape)\n",
    "\n",
    "  # Central crop the image.\n",
    "  image = tf.image.resize_with_crop_or_pad(image, target_dim, target_dim)\n",
    "\n",
    "  return image\n",
    "\n",
    "# Load the input images.\n",
    "content_path = tf.keras.utils.get_file('belfry.jpg','https://storage.googleapis.com/khanhlvg-public.appspot.com/arbitrary-style-transfer/belfry-2611573_1280.jpg')\n",
    "style_path = tf.keras.utils.get_file('style23.jpg','https://storage.googleapis.com/khanhlvg-public.appspot.com/arbitrary-style-transfer/style23.jpg')\n",
    "content_image = load_img(content_path)\n",
    "style_image = load_img(style_path)\n",
    "\n",
    "# Preprocess the input images.\n",
    "preprocessed_content_image = preprocess_image(content_image, 384)\n",
    "preprocessed_style_image = preprocess_image(style_image, 256)\n",
    "\n",
    "print('Style Image Shape:', preprocessed_style_image.shape)\n",
    "print('Content Image Shape:', preprocessed_content_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load some Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load example images  { display-mode: \"form\" }\n",
    "\n",
    "content_image_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Golden_Gate_Bridge_from_Battery_Spencer.jpg/640px-Golden_Gate_Bridge_from_Battery_Spencer.jpg'  # @param {type:\"string\"}\n",
    "style_image_url = 'https://upload.wikimedia.org/wikipedia/commons/0/0a/The_Great_Wave_off_Kanagawa.jpg'  # @param {type:\"string\"}\n",
    "output_image_size = 256  # @param {type:\"integer\"}\n",
    "\n",
    "# The content image size can be arbitrary.\n",
    "content_img_size = (output_image_size, output_image_size)\n",
    "# The style prediction model was trained with image size 256 and it's the \n",
    "# recommended image size for the style image (though, other sizes work as \n",
    "# well but will lead to different results).\n",
    "style_img_size = (256, 256)  # Recommended to keep it at 256.\n",
    "\n",
    "content_image = load_image(content_image_url, content_img_size)\n",
    "style_image = load_image(style_image_url, style_img_size)\n",
    "print('Content image shape: ', content_image.shape)\n",
    "style_image = tf.nn.avg_pool(style_image, ksize=[3,3], strides=[1,1], padding='SAME')\n",
    "print('Content image shape: ', content_image.shape)\n",
    "show_n([content_image, style_image], ['Content image', 'Style image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TF Hub module.\n",
    "\n",
    "hub_handle = 'https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2'\n",
    "hub_module = hub.load(hub_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the style transfer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stylize content image with given style image.\n",
    "# This is pretty fast within a few milliseconds on a GPU.\n",
    "\n",
    "outputs = hub_module(tf.constant(content_image), tf.constant(style_image))\n",
    "stylized_image = outputs[0]\n",
    "# Visualize input images and the generated stylized image.\n",
    "\n",
    "show_n([content_image, style_image, stylized_image], titles=['Original content image', 'Style image', 'Stylized image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transform with TFLIte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download The Style Transform and Style Prediction TFLite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are running the model on a smartphone, you can use the following code to download the model files and load them into the model.\n",
    "# https://github.com/tensorflow/tensorflow/issues/21698#issuecomment-414764709\n",
    "style_predict_path = tf.keras.utils.get_file('style_predict.tflite', 'https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/int8/prediction/1?lite-format=tflite')\n",
    "style_transform_path = tf.keras.utils.get_file('style_transform.tflite', 'https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/int8/transfer/1?lite-format=tflite')\n",
    "#style_predict_path = tf.keras.utils.get_file('style_predict.tflite', 'https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/int8/prediction/1?lite-format=tflite')\n",
    "#style_transform_path = tf.keras.utils.get_file('style_transform.tflite', 'https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/int8/transfer/1?lite-format=tflite')\n",
    "#style_predict_path = \"magenta_arbitrary-image-stylization-v1-256_fp16_prediction_1.tflite\"\n",
    "#style_transform_path = \"magenta_arbitrary-image-stylization-v1-256_fp16_transfer_1.tflite\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Predict with TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run style prediction on preprocessed style image.\n",
    "def run_style_predict(preprocessed_style_image):\n",
    "  # Load the model.\n",
    "  interpreter = tf.lite.Interpreter(model_path=style_predict_path)\n",
    "\n",
    "  # Set model input.\n",
    "  interpreter.allocate_tensors()\n",
    "  input_details = interpreter.get_input_details()\n",
    "  interpreter.set_tensor(input_details[0][\"index\"], preprocessed_style_image)\n",
    "\n",
    "  # Calculate style bottleneck.\n",
    "  interpreter.invoke()\n",
    "  style_bottleneck = interpreter.tensor(\n",
    "      interpreter.get_output_details()[0][\"index\"]\n",
    "      )()\n",
    "\n",
    "  return style_bottleneck\n",
    "\n",
    "# Calculate style bottleneck for the preprocessed style image.\n",
    "style_bottleneck = run_style_predict(preprocessed_style_image)\n",
    "print('Style Bottleneck Shape:', style_bottleneck.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transform with TFLIte method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, title=None):\n",
    "  if len(image.shape) > 3:\n",
    "    image = tf.squeeze(image, axis=0)\n",
    "\n",
    "  plt.imshow(image)\n",
    "  if title:\n",
    "    plt.title(title)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(preprocessed_content_image, 'Content Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(preprocessed_style_image, 'Style Image')\n",
    "# Run style transform on preprocessed style image\n",
    "def run_style_transform(style_bottleneck, preprocessed_content_image):\n",
    "  # Load the model.\n",
    "  interpreter = tf.lite.Interpreter(model_path=style_transform_path)\n",
    "\n",
    "  # get model input.\n",
    "  input_details = interpreter.get_input_details()\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  # Set model inputs.\n",
    "  interpreter.set_tensor(input_details[0][\"index\"], preprocessed_content_image)\n",
    "  interpreter.set_tensor(input_details[1][\"index\"], style_bottleneck)\n",
    "  interpreter.invoke()\n",
    "\n",
    "  # Transform content image.\n",
    "  stylized_image = interpreter.tensor(\n",
    "      interpreter.get_output_details()[0][\"index\"]\n",
    "      )()\n",
    "\n",
    "  return stylized_image\n",
    "def run_style_transform_without_loading_the_model(interpreter , style_bottleneck, preprocessed_content_image):\n",
    "\n",
    "  # Set model inputs.\n",
    "  interpreter.set_tensor(input_details[0][\"index\"], preprocessed_content_image)\n",
    "  interpreter.set_tensor(input_details[1][\"index\"], style_bottleneck)\n",
    "  interpreter.invoke()\n",
    "\n",
    "  # Transform content image.\n",
    "  stylized_image = interpreter.tensor(\n",
    "      interpreter.get_output_details()[0][\"index\"]\n",
    "      )()\n",
    "\n",
    "  return stylized_image\n",
    "# Stylize the content image using the style bottleneck.\n",
    "#stylized_image = run_style_transform(style_bottleneck, preprocessed_content_image)\n",
    "\n",
    "# Visualize the output.\n",
    "imshow(stylized_image, 'Stylized Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set webcam UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set webcam settings\n",
    "width = 500\n",
    "height = 500\n",
    "cam = cv2.VideoCapture(0)\n",
    "cam.set(3, width)\n",
    "cam.set(4, height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFLITE single Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "## No more interpreter called anymore\n",
    "# Load the model  The model path is then fed to the Interpreter class constructor for loading it. The loaded model is returned in the interpreter variable.\n",
    "interpreter = tf.lite.Interpreter(model_path=style_transform_path)\n",
    "# Set model input.\n",
    "#After the model is loaded, the allocate_tensors() method is called for allocating memory for the input and output tensors.\n",
    "interpreter.allocate_tensors()\n",
    "infomation_about_input = interpreter.get_input_details()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of CPU cores is 8\n",
      "The signatures are {}\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.37983868700030143 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.16626453100070648 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.1849704650012427 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.19921655799953442 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.24969463600064046 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.1586599339989334 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.24252505699951143 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.24459968600058346 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.16221958200003428 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.20958504100053688 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.23503112599973974 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.1890774000003148 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.1637844849992689 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.22945402200093667 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.20037602000047627 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.18951178300085303 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.178340032000051 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.18585653400077717 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.21109137700113934 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.18977423300020746 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.1789844989998528 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.2103038959994592 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.17713905799973872 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.1827253400006157 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.26987820000067586 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.26691756199943484 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.38682257300024503 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.29004156299924944 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.2774431199995888 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n",
      "Time taken to run the inference is 0.33637626500058104 seconds\n",
      "The image is 1 pixels high and 384 pixels wide\n",
      "The image daa type is float32\n",
      "(480, 640, 3)\n",
      "The image is 480 pixels high and 640 pixels wide\n",
      "The image data type is uint8\n",
      "The shape of the preprocessed image is (1, 384, 384, 3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22197/1240122457.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"index\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_bottleneck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# run the inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Time taken to run the inference is {end - start} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/neural_style_transfer/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    915\u001b[0m     \"\"\"\n\u001b[1;32m    916\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Main loop\n",
    "## No more interpreter called anymore\n",
    "# Load the model.\n",
    "print(f\"The number of CPU cores is {os.cpu_count()}\")\n",
    "## The Interpreter class constructor takes the model path as an argument for loading the model.\n",
    "interpreter = tf.lite.Interpreter(model_path=style_transform_path , \n",
    "                                  num_threads = os.cpu_count())\n",
    "signatures = interpreter.get_signature_list()\n",
    "print(f\"The signatures are {signatures}\")\n",
    "# Set model input\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "interpreter.allocate_tensors()\n",
    "# Variables to calculate FPS\n",
    "counter, fps = 0, 0\n",
    "start_time = time.time()\n",
    "count = 0\n",
    "# Visualization parameters\n",
    "_ROW_SIZE = 20  # pixels\n",
    "_LEFT_MARGIN = 24  # pixels\n",
    "_TEXT_COLOR = (0, 0, 255)  # red\n",
    "_FONT_SIZE = 1\n",
    "_FONT_THICKNESS = 1\n",
    "_FPS_AVERAGE_FRAME_COUNT = 10\n",
    "with torch.no_grad():\n",
    "    while True:\n",
    "        # Get webcam input\n",
    "        ret_val, img = cam.read()\n",
    "        counter += 1\n",
    "        # Mirror \n",
    "        img = cv2.flip(img, 1)\n",
    "        print(img.shape)\n",
    "        print(f\"The image is {img.shape[0]} pixels high and {img.shape[1]} pixels wide\")\n",
    "        print(f\"The image data type is {img.dtype}\")\n",
    "        \n",
    "        # Preprocess the images\n",
    "        tf_image = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        tf_image = tf_image[tf.newaxis, :]\n",
    "        preprocess_video_image = preprocess_image(tf_image, 384)\n",
    "        print(f\"The shape of the preprocessed image is {preprocess_video_image.shape}\")\n",
    "        \n",
    "        # Load the model inputs \n",
    "        start = time.perf_counter() \n",
    "        #stylized_image = run_style_transform_without_loading_the_model( interpreter,  style_bottleneck, preprocess_video_image) \n",
    "        # Set model inputs.\n",
    "        # input_details[0]['index'] = the index which accepts the input\n",
    "        interpreter.set_tensor(input_details[0][\"index\"], preprocess_video_image)\n",
    "        interpreter.set_tensor(input_details[1][\"index\"], style_bottleneck)\n",
    "        # run the inference\n",
    "        interpreter.invoke()\n",
    "        end = time.perf_counter()\n",
    "        print(f\"Time taken to run the inference is {end - start} seconds\")\n",
    "        # Calculate the FPS\n",
    "        if counter % _FPS_AVERAGE_FRAME_COUNT == 0:\n",
    "            end_time = time.time()\n",
    "            fps = _FPS_AVERAGE_FRAME_COUNT / (end_time - start_time)\n",
    "            start_time = time.time()\n",
    "\n",
    "        stylized_image = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "        # Check if the input type is quantized, then rescale input data to uint8\n",
    "        ## Conver the styleized image into uint8 numpy array that can feed into cv2 video camers\n",
    "        print(f\"The image is {stylized_image.shape[0]} pixels high and {stylized_image.shape[1]} pixels wide\")\n",
    "        print(f\"The image daa type is {stylized_image.dtype}\")\n",
    "        # Display result\n",
    "        cv2.imshow('my webcam',  stylized_image[0])\n",
    "        if cv2.waitKey(1) == 27: \n",
    "            break # esc to quit\n",
    "# Free-up memories\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.16 ('neural_style_transfer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10b5c7539df14ee8f298b6464fcc5d8f1d5a3dbba430b9c328588cb158ac4f0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
